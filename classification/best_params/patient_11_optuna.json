{
    "hidden_size": 150,
    "num_lstm_layers": 2,
    "num_fc_layers": 2,
    "dropout_prob": 0.5,
    "weight_decay": 8.011288280919354e-05,
    "learning_rate": 0.0006203279374908185,
    "batch_size": 8,
    "epochs": 30
}